# CSV Chunking Optimizer Pro API

A comprehensive CSV processing, chunking, embedding, and retrieval system with FastAPI backend and responsive frontend.

## ğŸš€ Features

### **Complete Processing Pipeline**
- **Preprocessing**: Data validation, type conversion, null handling, text processing
- **Chunking**: 4 methods (Fixed, Recursive, Document-based, Semantic)  
- **Embedding**: 2 models (all-MiniLM-L6-v2, BAAI/bge-small-en-v1.5)
- **Storage**: ChromaDB + FAISS vector databases
- **Retrieval**: 3 similarity metrics (cosine, dot product, euclidean)

### **Multi-Layer API**
- **Layer 1**: Fast mode with auto-optimized defaults
- **Layer 2**: Config mode with medium customization
- **Layer 3**: Deep config mode with full control
- **Unified API**: Single endpoint for company integration

### **File Downloads (Option B)**
- `chunks.csv` - All processed chunks
- `embeddings.json` - Vector embeddings with metadata
- `metadata.json` - Complete processing metadata
- `summary.json` - Processing summary and stats
- `results.zip` - All files in one archive

## ğŸ“ Project Structure

```
csv-chunking-optimizer-pro/
â”œâ”€â”€ frontend/                   # Responsive UI
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ script.js
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ backend/                    # Core processing
â”‚   â”œâ”€â”€ api/                   # FastAPI layer
â”‚   â”‚   â”œâ”€â”€ main.py           # FastAPI application
â”‚   â”‚   â”œâ”€â”€ models.py         # Pydantic models
â”‚   â”‚   â””â”€â”€ routes/           # API endpoints
â”‚   â”œâ”€â”€ core/                 # Processing modules
â”‚   â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”‚   â”œâ”€â”€ chunking.py
â”‚   â”‚   â”œâ”€â”€ embedding.py
â”‚   â”‚   â”œâ”€â”€ storing.py
â”‚   â”‚   â””â”€â”€ retrieval.py
â”‚   â”œâ”€â”€ services/             # Business logic
â”‚   â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”‚   â”œâ”€â”€ file_handler.py
â”‚   â”‚   â””â”€â”€ response_builder.py
â”‚   â””â”€â”€ utils/                # Utilities
â”œâ”€â”€ config/                   # Configuration
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ logging.py
â””â”€â”€ scripts/                  # Utility scripts
    â”œâ”€â”€ start_server.py
    â””â”€â”€ cleanup.py
```

## ğŸ› ï¸ Installation

### **1. Clone Repository**
```bash
git clone <repository-url>
cd csv-chunking-optimizer-pro
```

### **2. Install Dependencies**
```bash
pip install -r requirements.txt
```

### **3. Download spaCy Model**
```bash
python -m spacy download en_core_web_sm
```

### **4. Start Full Stack (Frontend + Backend)**
```bash
python scripts/start_full_stack.py
```

**Or start individually:**
```bash
# Backend only
python scripts/start_server.py

# Frontend only  
python scripts/start_frontend.py
```

## ğŸ“– API Documentation

### **Quick Start**
- **API Docs**: http://localhost:8000/api/docs
- **Health Check**: http://localhost:8000/api/v1/health
- **API Info**: http://localhost:8000/api/v1/info

### **Layer APIs (for UI)**

#### **Layer 1 - Fast Mode**
```bash
POST /api/v1/layer1/process
Content-Type: application/json

{
    "csv_data": "base64_encoded_csv",
    "filename": "data.csv"
}
```

#### **Layer 2 - Config Mode**
```bash
POST /api/v1/layer2/process
Content-Type: application/json

{
    "csv_data": "base64_encoded_csv",
    "filename": "data.csv",
    "chunking_method": "semantic",
    "embedding_model": "all-MiniLM-L6-v2",
    "batch_size": 64
}
```

#### **Layer 3 - Deep Config Mode**
```bash
POST /api/v1/layer3/process
Content-Type: application/json

{
    "csv_data": "base64_encoded_csv",
    "filename": "data.csv",
    "preprocessing": {
        "type_conversions": {"age": "numeric"},
        "null_handling": {"column1": "mean"}
    },
    "chunking": {
        "method": "document_based",
        "key_column": "id",
        "token_limit": 2000
    },
    "embedding": {
        "model": "BAAI/bge-small-en-v1.5",
        "batch_size": 64
    },
    "storage": {
        "type": "faiss",
        "similarity_metric": "cosine"
    }
}
```

### **Unified API (for Companies)**

```bash
POST /api/v1/process-csv
Content-Type: application/json

{
    "csv_data": "base64_encoded_csv",
    "filename": "data.csv",
    "layer_mode": "deep",
    "chunking": {
        "method": "semantic",
        "n_clusters": 5
    },
    "embedding": {
        "model": "all-MiniLM-L6-v2"
    }
}
```

### **Response Format (Option B)**
```json
{
    "success": true,
    "processing_id": "uuid-123",
    "timestamp": "2025-09-28T14:30:00Z",
    "processing_summary": {
        "layer_mode": "deep",
        "processing_time_seconds": 15.43,
        "input_data": {
            "total_rows": 1000,
            "total_columns": 5
        },
        "chunking_results": {
            "method": "semantic",
            "total_chunks": 25
        },
        "embedding_results": {
            "model_used": "all-MiniLM-L6-v2",
            "vector_dimension": 384
        }
    },
    "download_links": {
        "chunks_csv": {
            "url": "/api/v1/download/chunks_uuid-123.csv",
            "size_bytes": 145230,
            "expires_at": "2025-09-29T14:30:00Z"
        },
        "embeddings_json": {
            "url": "/api/v1/download/embeddings_uuid-123.json",
            "size_bytes": 2341567,
            "expires_at": "2025-09-29T14:30:00Z"
        },
        "metadata_json": {
            "url": "/api/v1/download/metadata_uuid-123.json",
            "size_bytes": 8945,
            "expires_at": "2025-09-29T14:30:00Z"
        },
        "results_zip": {
            "url": "/api/v1/download/results_uuid-123.zip",
            "size_bytes": 2495742,
            "expires_at": "2025-09-29T14:30:00Z"
        }
    },
    "search_endpoint": "/api/v1/search/uuid-123"
}
```

### **Search API**

```bash
POST /api/v1/search/{processing_id}
Content-Type: application/json

{
    "query": "customer information",
    "model_name": "all-MiniLM-L6-v2",
    "top_k": 5,
    "similarity_metric": "cosine"
}
```

## ğŸ§ª Testing

### **Test Backend**
```bash
python test_backend.py
```

### **Test API**
```bash
# Start server first
python scripts/start_server.py

# In another terminal, test endpoints
curl -X GET http://localhost:8000/api/v1/health
curl -X GET http://localhost:8000/api/v1/info
```

## ğŸ”§ Configuration

### **Environment Variables**
Create `.env` file:
```env
HOST=0.0.0.0
PORT=8000
DEBUG=True
MAX_FILE_SIZE_MB=100
FILE_RETENTION_HOURS=24
```

### **Settings**
Modify `config/settings.py` for advanced configuration.

## ğŸ—ï¸ Architecture

### **Processing Flow**
```
CSV Upload â†’ Preprocessing â†’ Chunking â†’ Embedding â†’ Storage â†’ Search
```

### **API Layers**
```
Frontend UI â†’ Layer APIs â†’ Services â†’ Core Modules
Companies â†’ Unified API â†’ Services â†’ Core Modules
```

### **Storage Options**
- **ChromaDB**: Easy setup, good for development
- **FAISS**: High performance, production ready

### **Similarity Metrics**
- **Cosine**: Angle between vectors (0-1)
- **Dot Product**: Vector dot product (unbounded)
- **Euclidean**: Distance converted to similarity (0-1)

## ğŸ“Š Performance

### **Supported Scales**
- **File Size**: Up to 100MB CSV files
- **Processing**: 1000s of rows per second
- **Chunking**: Multiple methods for optimal results
- **Search**: Sub-second response times

### **Optimization Tips**
1. Use **Layer 1** for fastest processing
2. Choose **FAISS** for large datasets (>10K chunks)
3. Use **batch_size=64** for better embedding performance
4. **Semantic chunking** works best for similar content

## ğŸš€ Deployment

### **Development**
```bash
python scripts/start_server.py
```

### **Production**
```bash
uvicorn backend.api.main:app --host 0.0.0.0 --port 8000 --workers 4
```

### **Docker** (Future)
```bash
docker build -t csv-optimizer .
docker run -p 8000:8000 csv-optimizer
```

## ğŸ¤ Company Integration

### **Single API Endpoint**
Use `/api/v1/process-csv` for complete processing pipeline.

### **Response Format**
Option B: File Downloads + JSON metadata

### **Integration Steps**
1. Encode CSV as base64
2. POST to `/api/v1/process-csv`
3. Download files from provided URLs
4. Use search endpoint for queries

### **Example Integration**
```python
import requests
import base64

# Encode CSV
with open('data.csv', 'rb') as f:
    csv_data = base64.b64encode(f.read()).decode()

# Process
response = requests.post('http://api.example.com/api/v1/process-csv', json={
    "csv_data": csv_data,
    "filename": "data.csv",
    "layer_mode": "fast"
})

result = response.json()
print(f"Processing ID: {result['processing_id']}")
print(f"Download links: {result['download_links']}")
```

## ğŸ“ Support

- **API Documentation**: `/api/docs`
- **Health Status**: `/api/v1/health`
- **File Cleanup**: `python scripts/cleanup.py`

## ğŸ”„ Version History

- **v1.0.0**: Complete FastAPI integration with all features

---

**CSV Chunking Optimizer Pro** - Production-ready CSV processing API with comprehensive chunking, embedding, and retrieval capabilities.
